{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d2fbf-c44e-4607-82ca-eaf4a2ca4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/16 02:30:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/16 02:58:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "[Stage 32:=================================================>  (1900 + 8) / 2000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8322\n",
      "Test F1-score: 0.7607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon_Reviews_Recommender_Optimized\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2000\") \\\n",
    "    .config(\"spark.default.parallelism\", \"2000\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "PARQUET_PATH = \"reviews_final_parquet\"\n",
    "df = spark.read.parquet(PARQUET_PATH)\n",
    "\n",
    "df_spark = df.withColumn(\n",
    "    \"label\", \n",
    "    F.when((F.col(\"overall\") >= 1) & (F.col(\"overall\") <= 2.5), 0.0)\n",
    "     .when((F.col(\"overall\") > 2.5) & (F.col(\"overall\") <= 3.5), 1.0)\n",
    "     .otherwise(2.0)\n",
    ")\n",
    "\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"lemmatized_tokens\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Multiclass Logistic Regression\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\", \n",
    "    maxIter=10, \n",
    "    regParam=0.01, \n",
    "    elasticNetParam=1.0, \n",
    "    family=\"multinomial\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[ hashingTF, idf, lr])\n",
    "\n",
    "train_df, test_df = df_spark.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119d1a1-4783-49ff-991f-6d052071d76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Saving PipelineModel to logreg_sentiment_pipeline_v1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Model saved.\n",
      "--- Loading PipelineModel from logreg_sentiment_pipeline_v1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Model loaded.\n",
      "--- Verifying Loaded Model on Test Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 0.8322\n",
      "Loaded Model Accuracy: 0.8322\n",
      "Verification Passed: Loaded model behaves exactly like the original.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------------------------------------------------+\n",
      "|label|prediction|probability                                                  |\n",
      "+-----+----------+-------------------------------------------------------------+\n",
      "|2.0  |2.0       |[0.046960655081238194,0.05170117521505912,0.9013381697037026]|\n",
      "|2.0  |2.0       |[0.08299363148734924,0.07983364357356251,0.8371727249390882] |\n",
      "|2.0  |2.0       |[0.045329722850059324,0.07387914078544117,0.8807911363644996]|\n",
      "|2.0  |2.0       |[0.032056267809961465,0.03575619654839257,0.9321875356416459]|\n",
      "|2.0  |2.0       |[0.10927388228380479,0.09844390638973831,0.7922822113264569] |\n",
      "+-----+----------+-------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "model_path = \"logreg_sentiment_pipeline_v1\"\n",
    "print(f\"--- Saving PipelineModel to {model_path} ---\")\n",
    "\n",
    "model.write().overwrite().save(model_path)\n",
    "print(\"Success: Model saved.\")\n",
    "\n",
    "print(f\"--- Loading PipelineModel from {model_path} ---\")\n",
    "loaded_model = PipelineModel.load(model_path)\n",
    "print(\"Success: Model loaded.\")\n",
    "\n",
    "print(\"--- Verifying Loaded Model on Test Data ---\")\n",
    "\n",
    "predictions_loaded = loaded_model.transform(test_df)\n",
    "\n",
    "evaluator_verify = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "acc_verify = evaluator_verify.evaluate(predictions_loaded, {evaluator_verify.metricName: \"accuracy\"})\n",
    "\n",
    "print(f\"Original Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Loaded Model Accuracy: {acc_verify:.4f}\")\n",
    "\n",
    "if accuracy == acc_verify:\n",
    "    print(\"Verification Passed: Loaded model behaves exactly like the original.\")\n",
    "else:\n",
    "    print(\"Verification Warning: Metrics differ slightly (this is rare).\")\n",
    "\n",
    "predictions_loaded.select(\"label\", \"prediction\", \"probability\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd799d-980f-4900-b131-23d25a78736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+----------+\n",
      "|lemmatized_tokens                                                     |prediction|\n",
      "+----------------------------------------------------------------------+----------+\n",
      "|[absolutely, hate, book, plot, be, non, existent, character, be, flat]|2.0       |\n",
      "|[amazing, journey, not, can, put, down, last, page]                   |2.0       |\n",
      "|[hate, awful]                                                         |2.0       |\n",
      "+----------------------------------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon_Reviews_Recommender_Optimized\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2000\") \\\n",
    "    .config(\"spark.default.parallelism\", \"2000\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "model_path = \"logreg_sentiment_pipeline_v1\"\n",
    "\n",
    "loaded_model = PipelineModel.load(model_path)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"originalText\", StringType(), True),\n",
    "    StructField(\"lemmatized_tokens\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"I absolutely hated this book...\", [\"absolutely\", \"hate\", \"book\", \"plot\", \"be\", \"non\", \"existent\", \"character\", \"be\", \"flat\"]),\n",
    "    (\"What an amazing journey!...\", [\"amazing\", \"journey\", \"not\", \"can\", \"put\", \"down\", \"last\", \"page\"]),\n",
    "    (\"I hate it! It's awful\", [\"hate\", \"awful\"])\n",
    "]\n",
    "\n",
    "test_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "results = loaded_model.transform(test_df)\n",
    "\n",
    "results.select(\"lemmatized_tokens\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98338b-5d25-41a9-ac78-d6aefb3e6957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/16 07:25:31 WARN DAGScheduler: Broadcasting large task binary with size 1127.2 KiB\n",
      "26/01/16 08:00:25 WARN DAGScheduler: Broadcasting large task binary with size 1128.4 KiB\n",
      "26/01/16 08:00:35 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/01/16 08:00:36 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 08:40:27 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 08:40:47 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 08:45:56 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 08:46:08 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 08:50:45 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 08:50:50 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 08:55:35 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 08:56:02 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 09:01:43 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 09:01:55 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 09:06:58 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 09:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 09:12:25 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 09:12:30 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 09:16:08 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 09:16:17 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 09:19:59 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 09:20:07 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 09:25:02 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 09:25:26 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 09:31:46 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 09:31:56 WARN DAGScheduler: Broadcasting large task binary with size 1127.8 KiB\n",
      "26/01/16 09:37:41 WARN DAGScheduler: Broadcasting large task binary with size 1128.9 KiB\n",
      "26/01/16 09:37:56 WARN DAGScheduler: Broadcasting large task binary with size 1128.5 KiB\n",
      "26/01/16 09:38:02 WARN BlockManagerMaster: Failed to remove RDD 49 - Block rdd_49_1062 does not exist\n",
      "org.apache.spark.SparkException: Block rdd_49_1062 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:544)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2095)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2061)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4(BlockManager.scala:1997)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4$adapted(BlockManager.scala:1997)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeRdd(BlockManager.scala:1997)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$2(BlockManagerStorageEndpoint.scala:53)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "26/01/16 09:38:02 ERROR ContextCleaner: Error cleaning RDD 49\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.removeRdd(BlockManagerMaster.scala:196)\n",
      "\tat org.apache.spark.SparkContext.unpersistRDD(SparkContext.scala:2075)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupRDD(ContextCleaner.scala:225)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:200)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1356)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "Caused by: org.apache.spark.SparkException: Block rdd_49_1062 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:544)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2095)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2061)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4(BlockManager.scala:1997)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4$adapted(BlockManager.scala:1997)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeRdd(BlockManager.scala:1997)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$2(BlockManagerStorageEndpoint.scala:53)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "26/01/16 09:47:09 WARN DAGScheduler: Broadcasting large task binary with size 1128.5 KiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8377\n",
      "Test F1-score: 0.7740\n",
      "--- Saving PipelineModel to logreg_sentiment_pipeline_v2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/16 09:56:19 WARN TaskSetManager: Stage 45 contains a task of very large size (1052 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 51:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, NGram, SQLTransformer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon_Reviews_Recommender_Optimized\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2000\") \\\n",
    "    .config(\"spark.default.parallelism\", \"2000\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "PARQUET_PATH = \"reviews_final_parquet\"\n",
    "df = spark.read.parquet(PARQUET_PATH)\n",
    "\n",
    "df_spark = df.withColumn(\n",
    "    \"label\", \n",
    "    F.when((F.col(\"overall\") >= 1) & (F.col(\"overall\") <= 2.5), 0.0)\n",
    "     .when((F.col(\"overall\") > 2.5) & (F.col(\"overall\") <= 3.5), 1.0)\n",
    "     .otherwise(2.0)\n",
    ")\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"lemmatized_tokens\", outputCol=\"bigrams\")\n",
    "\n",
    "combiner = SQLTransformer(\n",
    "    statement=\"SELECT *, concat(lemmatized_tokens, bigrams) AS all_tokens FROM __THIS__\"\n",
    ")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"all_tokens\", outputCol=\"rawFeatures\", numFeatures=2**16)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\", \n",
    "    maxIter=10, \n",
    "    regParam=0.01, \n",
    "    elasticNetParam=1.0, \n",
    "    family=\"multinomial\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[ngram, combiner, hashingTF, idf, lr])\n",
    "\n",
    "train_df, test_df = df_spark.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1-score: {f1:.4f}\")\n",
    "\n",
    "model_path = \"logreg_sentiment_pipeline_v2\"\n",
    "print(f\"--- Saving PipelineModel to {model_path} ---\")\n",
    "\n",
    "model.write().save(model_path)\n",
    "print(\"Success: Model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
