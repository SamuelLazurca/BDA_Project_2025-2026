{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon_Reviews_Recommender_Optimized\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2000\") \\\n",
    "    .config(\"spark.default.parallelism\", \"2000\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"checkpoints_als\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b68a9-65ff-4711-87ab-afbef8edd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = \"reviews_final_parquet\"\n",
    "df = spark.read.parquet(PARQUET_PATH)\n",
    "\n",
    "def load_and_clean_data(df):\n",
    "    item_counts = df.groupBy(\"asin\").count().filter(F.col(\"count\") >= 5)\n",
    "    user_counts = df.groupBy(\"reviewerID\").count().filter(F.col(\"count\") >= 5)\n",
    "    \n",
    "    df_clean = df.join(item_counts, \"asin\", \"left_semi\") \\\n",
    "                 .join(user_counts, \"reviewerID\", \"left_semi\")\n",
    "    return df_clean\n",
    "\n",
    "print(\"--- Filtering Data ---\")\n",
    "df_spark = load_and_clean_data(df)\n",
    "\n",
    "\n",
    "def create_index_mapping(df, col_name, output_col):\n",
    "    print(f\"--- Indexing column: {col_name} ---\")\n",
    "    distinct_vals = df.select(col_name).distinct()\n",
    "    \n",
    "    mapping_rdd = distinct_vals.rdd.map(lambda r: r[0]).zipWithUniqueId()\n",
    "    \n",
    "    mapping_df = spark.createDataFrame(mapping_rdd, schema=[\"original_id\", \"numeric_id\"])\n",
    "    \n",
    "    mapping_path = f\"mappings/{col_name}_mapping\"\n",
    "    mapping_df.write.mode(\"overwrite\").parquet(mapping_path)\n",
    "    \n",
    "    return spark.read.parquet(mapping_path).withColumnRenamed(\"original_id\", col_name).withColumnRenamed(\"numeric_id\", output_col)\n",
    "\n",
    "user_mapping = create_index_mapping(df_spark, \"reviewerID\", \"user_id_index\")\n",
    "item_mapping = create_index_mapping(df_spark, \"asin\", \"asin_index\")\n",
    "\n",
    "print(\"--- Joining Indices back to Data ---\")\n",
    "df_indexed = df_spark.join(user_mapping, on=\"reviewerID\", how=\"inner\") \\\n",
    "                     .join(item_mapping, on=\"asin\", how=\"inner\")\n",
    "\n",
    "df_final = df_indexed.select(\n",
    "    F.col(\"user_id_index\").cast(\"integer\"),\n",
    "    F.col(\"asin_index\").cast(\"integer\"),\n",
    "    F.col(\"overall\").cast(\"float\")\n",
    ")\n",
    "\n",
    "print(\"--- Splitting Data ---\")\n",
    "(training_data, test_data) = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "training_data.persist(StorageLevel.DISK_ONLY)\n",
    "test_data.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(f\"Training Count: {training_data.count()}\")\n",
    "print(f\"Test Count: {test_data.count()}\")\n",
    "\n",
    "print(\"--- Calculating User Means ---\")\n",
    "user_means = training_data.groupBy(\"user_id_index\") \\\n",
    "    .agg(F.avg(\"overall\").alias(\"user_mean\"))\n",
    "\n",
    "training_centered = training_data.join(user_means, on=\"user_id_index\", how=\"inner\")\n",
    "training_centered = training_centered.withColumn(\"centered_rating\", F.col(\"overall\") - F.col(\"user_mean\"))\n",
    "\n",
    "training_centered_path = \"tmp_training_centered\"\n",
    "training_centered.write.mode(\"overwrite\").parquet(training_centered_path)\n",
    "training_centered_ready = spark.read.parquet(training_centered_path)\n",
    "\n",
    "print(\"--- Training ALS ---\")\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    rank=20, \n",
    "    userCol=\"user_id_index\",\n",
    "    itemCol=\"asin_index\",\n",
    "    ratingCol=\"centered_rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=False,\n",
    "    checkpointInterval=2  \n",
    ")\n",
    "\n",
    "model = als.fit(training_centered_ready)\n",
    "print(\"--- Model Trained Successfully ---\")\n",
    "\n",
    "print(\"--- Evaluating ---\")\n",
    "test_with_mean = test_data.join(user_means, on=\"user_id_index\", how=\"inner\")\n",
    "predictions = model.transform(test_with_mean)\n",
    "\n",
    "final_predictions = predictions.withColumn(\n",
    "    \"final_prediction\", \n",
    "    F.col(\"prediction\") + F.col(\"user_mean\")\n",
    ")\n",
    "\n",
    "final_predictions = final_predictions.withColumn(\n",
    "    \"final_prediction_clipped\", \n",
    "    F.when(F.col(\"final_prediction\") > 5, 5.0)\n",
    "     .when(F.col(\"final_prediction\") < 1, 1.0)\n",
    "     .otherwise(F.col(\"final_prediction\"))\n",
    ")\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", \n",
    "    labelCol=\"overall\", \n",
    "    predictionCol=\"final_prediction_clipped\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(final_predictions)\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9298d1e",
   "metadata": {},
   "source": [
    "Training Count: 60149963\n",
    "Test Count: 15036389\n",
    "RMSE: 1.027357238789061"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
