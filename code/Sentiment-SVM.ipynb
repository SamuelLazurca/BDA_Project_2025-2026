{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca8787-6b89-45f1-8961-0ff86d768c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install recbole\n",
    "# !pip install ray\n",
    "# !pip install \"numpy<2.0\"\n",
    "# !pip install kmeans_pytorch\n",
    "# !pip install \"scipy<1.12\"\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install pyarrow\n",
    "# !pip install tensorflow\n",
    "# !pip install spark-nlp\n",
    "# !pip install emoji\n",
    "# !pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51f25e-6fe2-4592-ac1a-a2b46277f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Results: SVM on video-games:\n",
    "\n",
    "SVM Accuracy: 0.9487\n",
    "SVM F1-score: 0.9396\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89bfa6-9ad0-4c4f-b4fb-8d1fe5fa6943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/14 17:04:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Load and filtering done.\n",
      ">>> Începe antrenarea modelului...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/14 17:38:15 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> S-a terminat antrenarea modelului.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 223:====================================================>(399 + 1) / 400]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratețe finală: 0.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, NGram\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sparknlp\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon_Sentiment_Analysis\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.default.parallelism\", \"400\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "BASE_DIR = '/home/ubuntu/data/aaucr/'\n",
    "file_path = os.path.join(BASE_DIR, 'Books_5.json')\n",
    "\n",
    "df = spark.read.json(file_path)\n",
    "df.select(\"asin\", \"reviewerID\", \"reviewText\", \"overall\").write.mode(\"overwrite\").parquet(\"data_amazon.parquet\")\n",
    "\n",
    "# CONFIGURATION FOR 2 WORKERS x 12GB\n",
    "PARQUET_PATH = \"data_amazon.parquet\"\n",
    "NUM_PARTITIONS = 400  \n",
    "NUM_FEATURES = 2**15  \n",
    "MAX_ITER = 20         \n",
    "REG_PARAM = 0.1\n",
    "\n",
    "df = spark.read.parquet(PARQUET_PATH).repartition(NUM_PARTITIONS)\n",
    "\n",
    "item_counts = df.groupBy(\"asin\").count().filter(\"count >= 5\").select(\"asin\")\n",
    "user_counts = df.groupBy(\"reviewerID\").count().filter(\"count >= 5\").select(\"reviewerID\")\n",
    "\n",
    "df = df.join(broadcast(item_counts), \"asin\") \\\n",
    "       .join(broadcast(user_counts), \"reviewerID\") \\\n",
    "       .select(\"reviewText\", \"overall\") \n",
    "\n",
    "df = df.filter(\"overall != 3\") \\\n",
    "       .withColumn(\"label\", F.when(F.col(\"overall\") >= 4, 1.0).otherwise(0.0)) \\\n",
    "       .withColumn(\"reviewText\", F.lower(F.col(\"reviewText\"))) \\\n",
    "       .withColumn(\"reviewText\", F.regexp_replace(\"reviewText\", r\"http\\S+|www\\S+|[^a-z\\s]\", \" \")) \\\n",
    "       .withColumn(\"reviewText\", F.trim(F.regexp_replace(\"reviewText\", r\"\\s+\", \" \"))) \\\n",
    "       .filter(F.length(\"reviewText\") > 10) \\\n",
    "       .select(\"reviewText\", \"label\")\n",
    "\n",
    "print(\">> Load and filtering done.\")\n",
    "\n",
    "# PIPELINE\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_clean\")\n",
    "hashingTF = HashingTF(inputCol=\"tokens_clean\", outputCol=\"rawFeatures\", numFeatures=NUM_FEATURES)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "svm = LinearSVC(maxIter=MAX_ITER, regParam=REG_PARAM, labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, svm])\n",
    "\n",
    "# TRAIN AND SPLIT\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\">>> Începe antrenarea modelului...\")\n",
    "spark.catalog.clearCache()\n",
    "model = pipeline.fit(train_df)\n",
    "print(\">>> S-a terminat antrenarea modelului.\")\n",
    "\n",
    "# EVALUATION\n",
    "predictions = model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Acuratețe finală: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c9f81-602a-4543-afd5-73fe09b37a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Se salvează modelul în: models/amazon_sentiment_svm_pipeline_books\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvare reușită!\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"models/amazon_sentiment_svm_pipeline_books\"\n",
    "\n",
    "print(f\">>> Se salvează modelul în: {MODEL_PATH}\")\n",
    "\n",
    "model.write().overwrite().save(MODEL_PATH)\n",
    "\n",
    "print(\"Salvare reușită!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc59f85-fe4a-4bdd-9468-cc2390c5d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 299:=================================================>   (257 + 6) / 275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+----------+\n",
      "|reviewText                                                                           |prediction|\n",
      "+-------------------------------------------------------------------------------------+----------+\n",
      "|I absolutely hated this book, the plot was non-existent and the characters were flat.|1.0       |\n",
      "|What an amazing journey! I couldn't put it down until the last page.                 |1.0       |\n",
      "|I hate it!                                                                           |1.0       |\n",
      "+-------------------------------------------------------------------------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(\"models/amazon_sentiment_svm_pipeline_books\")\n",
    "\n",
    "test_sentences = spark.createDataFrame([\n",
    "    (\"I absolutely hated this book, the plot was non-existent and the characters were flat.\",),\n",
    "    (\"What an amazing journey! I couldn't put it down until the last page.\",),\n",
    "    (\"I hate it! It's awful\",)\n",
    "], [\"reviewText\"])\n",
    "\n",
    "results = loaded_model.transform(test_sentences)\n",
    "results.select(\"reviewText\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccb8ea-ccd7-49ba-9d36-46ca55c766ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/15 20:39:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/15 20:39:49 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Load and filtering done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pozitive: 50048793, Negative: 5063240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noua dimensiune antrenare: 10126555\n",
      ">>> Începe antrenarea modelului...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 21:14:38 WARN DAGScheduler: Broadcasting large task binary with size 1114.8 KiB\n",
      "26/01/15 21:16:43 WARN DAGScheduler: Broadcasting large task binary with size 1115.9 KiB\n",
      "26/01/15 21:16:44 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/01/15 21:16:44 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:43 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:43 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:45 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:46 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:47 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:47 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:49 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:49 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:51 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:51 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:52 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:53 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:54 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:54 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:56 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:56 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:58 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:58 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:18:59 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:18:59 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:01 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:01 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:02 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:03 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:04 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:04 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:06 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:06 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:08 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:08 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:09 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:09 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:11 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:11 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:13 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:13 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:14 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:14 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:16 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:16 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:17 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:18 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:19 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:19 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:21 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:21 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:22 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:23 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:24 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:24 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:26 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:26 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:27 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:28 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:29 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:29 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:31 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:31 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:32 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:33 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:34 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:34 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:36 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:36 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:38 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:38 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:39 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:39 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:41 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:41 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:42 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:43 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:44 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:44 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:46 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:46 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:47 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:47 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:49 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "26/01/15 21:19:49 WARN DAGScheduler: Broadcasting large task binary with size 1115.3 KiB\n",
      "26/01/15 21:19:50 WARN DAGScheduler: Broadcasting large task binary with size 1116.5 KiB\n",
      "[Stage 146:====================================================>(797 + 3) / 800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> S-a terminat antrenarea modelului.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 21:32:41 WARN DAGScheduler: Broadcasting large task binary with size 1604.7 KiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratețe finală: 0.9058\n",
      ">>> Se salvează modelul în: models/amazon_sentiment_svm_big\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 21:37:13 WARN TaskSetManager: Stage 152 contains a task of very large size (1052 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvare reușită!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, NGram\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sparknlp\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon_Sentiment_Analysis\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.default.parallelism\", \"400\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# BASE_DIR = '/home/ubuntu/data/aaucr/'\n",
    "# file_path = os.path.join(BASE_DIR, 'Books_5.json')\n",
    "\n",
    "#df = spark.read.json(file_path)\n",
    "#df.select(\"asin\", \"reviewerID\", \"reviewText\", \"overall\").write.mode(\"overwrite\").parquet(\"data_amazon.parquet\")\n",
    "\n",
    "# CONFIGURATION\n",
    "PARQUET_PATH = \"reviews_final_parquet\"\n",
    "NUM_PARTITIONS = 400  \n",
    "NUM_FEATURES = 2**16  \n",
    "MAX_ITER = 20         \n",
    "REG_PARAM = 0.1\n",
    "\n",
    "df = spark.read.parquet(PARQUET_PATH) #.repartition(NUM_PARTITIONS)\n",
    "\n",
    "# PREPROCESSING\n",
    "df = df.filter(\"overall != 3\") \\\n",
    "       .withColumn(\"label\", F.when(F.col(\"overall\") >= 4, 1.0).otherwise(0.0)) \\\n",
    "       .select(\"lemmatized_tokens\", \"label\")\n",
    "\n",
    "print(\">> Load and filtering done.\")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"lemmatized_tokens\", outputCol=\"rawFeatures\", numFeatures=NUM_FEATURES)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "svm = LinearSVC(maxIter=MAX_ITER, regParam=REG_PARAM, labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, svm])\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "num_negatives = train_df.filter(F.col(\"label\") == 0.0).count()\n",
    "num_positives = train_df.filter(F.col(\"label\") == 1.0).count()\n",
    "\n",
    "print(f\"Pozitive: {num_positives}, Negative: {num_negatives}\")\n",
    "\n",
    "# DOWNSAMPLING\n",
    "fraction = num_negatives / num_positives\n",
    "pos_df = train_df.filter(F.col(\"label\") == 1.0).sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "neg_df = train_df.filter(F.col(\"label\") == 0.0)\n",
    "\n",
    "# UNION\n",
    "balanced_train_df = pos_df.union(neg_df).repartition(NUM_PARTITIONS)\n",
    "\n",
    "print(f\"Noua dimensiune antrenare: {balanced_train_df.count()}\")\n",
    "\n",
    "print(\">>> Începe antrenarea modelului...\")\n",
    "spark.catalog.clearCache()\n",
    "model = pipeline.fit(balanced_train_df)\n",
    "print(\">>> S-a terminat antrenarea modelului.\")\n",
    "\n",
    "# EVALUATION\n",
    "predictions = model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Acuratețe finală: {accuracy:.4f}\")\n",
    "\n",
    "MODEL_PATH = \"models/amazon_sentiment_svm_big\"\n",
    "\n",
    "print(f\">>> Se salvează modelul în: {MODEL_PATH}\")\n",
    "\n",
    "# SAVE\n",
    "model.write().overwrite().save(MODEL_PATH)\n",
    "\n",
    "print(\"Salvare reușită!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e412b9c-6596-4222-b938-cb0fbf96d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(MODEL_PATH)\n",
    "\n",
    "test_sentences = spark.createDataFrame([\n",
    "    (\"I absolutely hated this book, the plot was non-existent and the characters were flat.\",),\n",
    "    (\"What an amazing journey! I couldn't put it down until the last page.\",),\n",
    "    (\"I hate it! It's awful\",)\n",
    "], [\"reviewText\"])\n",
    "\n",
    "results = loaded_model.transform(test_sentences)\n",
    "results.select(\"reviewText\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72982c7f-e7fa-467b-b284-49fb96738a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 21:58:04 WARN DAGScheduler: Broadcasting large task binary with size 1581.9 KiB\n",
      "26/01/15 21:58:04 WARN DAGScheduler: Broadcasting large task binary with size 1581.9 KiB\n",
      "26/01/15 21:58:05 WARN DAGScheduler: Broadcasting large task binary with size 1581.9 KiB\n",
      "26/01/15 21:58:05 WARN DAGScheduler: Broadcasting large task binary with size 1581.9 KiB\n",
      "26/01/15 21:58:07 WARN DAGScheduler: Broadcasting large task binary with size 1581.9 KiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+----------+\n",
      "|lemmatized_tokens                                                     |prediction|\n",
      "+----------------------------------------------------------------------+----------+\n",
      "|[absolutely, hate, book, plot, be, non, existent, character, be, flat]|0.0       |\n",
      "|[amazing, journey, not, can, put, down, last, page]                   |1.0       |\n",
      "|[hate, awful]                                                         |0.0       |\n",
      "+----------------------------------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, LemmatizerModel\n",
    "from pyspark.ml import Pipeline as SparkNLP_Pipeline\n",
    "\n",
    "loaded_model = PipelineModel.load(MODEL_PATH)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"originalText\", StringType(), True),\n",
    "    StructField(\"lemmatized_tokens\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"I absolutely hated this book...\", [\"absolutely\", \"hate\", \"book\", \"plot\", \"be\", \"non\", \"existent\", \"character\", \"be\", \"flat\"]),\n",
    "    (\"What an amazing journey!...\", [\"amazing\", \"journey\", \"not\", \"can\", \"put\", \"down\", \"last\", \"page\"]),\n",
    "    (\"I hate it! It's awful\", [\"hate\", \"awful\"])\n",
    "]\n",
    "\n",
    "test_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "results = loaded_model.transform(test_df)\n",
    "\n",
    "results.select(\"lemmatized_tokens\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4444e3-4775-48aa-8130-e1d284fedb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Results on Books dataset (20+ GB) for the previous version (The \"Yes Man Problem\"):\n",
    "\n",
    "+-------------------------------------------------------------------------------------+----------+\n",
    "|reviewText                                                                           |prediction|\n",
    "+-------------------------------------------------------------------------------------+----------+\n",
    "|I absolutely hated this book, the plot was non-existent and the characters were flat.|1.0       |\n",
    "|What an amazing journey! I couldn't put it down until the last page.                 |1.0       |\n",
    "|I hate it! It's awful                                                                |0.0       |\n",
    "+-------------------------------------------------------------------------------------+----------+\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
