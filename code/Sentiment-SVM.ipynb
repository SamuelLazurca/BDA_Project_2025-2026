{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bb1fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, NGram\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sparknlp\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon_Sentiment_Analysis\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.default.parallelism\", \"400\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d0509",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "PARQUET_PATH = \"reviews_final_parquet\"\n",
    "NUM_PARTITIONS = 400  \n",
    "NUM_FEATURES = 2**16  \n",
    "MAX_ITER = 20         \n",
    "REG_PARAM = 0.1\n",
    "\n",
    "df = spark.read.parquet(PARQUET_PATH) #.repartition(NUM_PARTITIONS)\n",
    "\n",
    "# PREPROCESSING\n",
    "df = df.filter(\"overall != 3\") \\\n",
    "       .withColumn(\"label\", F.when(F.col(\"overall\") >= 4, 1.0).otherwise(0.0)) \\\n",
    "       .select(\"lemmatized_tokens\", \"label\")\n",
    "       # .withColumn(\"reviewText\", F.lower(F.col(\"reviewText\"))) \\\n",
    "       # .withColumn(\"reviewText\", F.regexp_replace(\"reviewText\", r\"http\\S+|www\\S+|[^a-z\\s]\", \" \")) \\\n",
    "       # .withColumn(\"reviewText\", F.trim(F.regexp_replace(\"reviewText\", r\"\\s+\", \" \"))) \\\n",
    "       # .filter(F.length(\"reviewText\") > 10) \\\n",
    "\n",
    "print(\">> Load and filtering done.\")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"lemmatized_tokens\", outputCol=\"rawFeatures\", numFeatures=NUM_FEATURES)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "svm = LinearSVC(maxIter=MAX_ITER, regParam=REG_PARAM, labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, svm])\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "num_negatives = train_df.filter(F.col(\"label\") == 0.0).count()\n",
    "num_positives = train_df.filter(F.col(\"label\") == 1.0).count()\n",
    "\n",
    "print(f\"Pozitive: {num_positives}, Negative: {num_negatives}\")\n",
    "\n",
    "# DOWNSAMPLING\n",
    "fraction = num_negatives / num_positives\n",
    "pos_df = train_df.filter(F.col(\"label\") == 1.0).sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "neg_df = train_df.filter(F.col(\"label\") == 0.0)\n",
    "\n",
    "# UNION\n",
    "balanced_train_df = pos_df.union(neg_df).repartition(NUM_PARTITIONS)\n",
    "\n",
    "print(f\"Noua dimensiune antrenare: {balanced_train_df.count()}\")\n",
    "\n",
    "print(\">>> Începe antrenarea modelului...\")\n",
    "spark.catalog.clearCache()\n",
    "model = pipeline.fit(balanced_train_df)\n",
    "print(\">>> S-a terminat antrenarea modelului.\")\n",
    "\n",
    "# EVALUATION\n",
    "predictions = model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Acuratețe finală: {accuracy:.4f}\")\n",
    "\n",
    "MODEL_PATH = \"models/amazon_sentiment_svm_big\"\n",
    "\n",
    "print(f\">>> Se salvează modelul în: {MODEL_PATH}\")\n",
    "\n",
    "# SAVE\n",
    "model.write().overwrite().save(MODEL_PATH)\n",
    "\n",
    "print(\"Salvare reușită!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12372c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pozitive: 50048793, Negative: 5063240\n",
    "                                                                                \n",
    "Noua dimensiune antrenare: 10126555\n",
    "\n",
    "Acuratețe finală: 0.9058\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b477a51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, LemmatizerModel\n",
    "from pyspark.ml import Pipeline as SparkNLP_Pipeline\n",
    "\n",
    "loaded_model = PipelineModel.load(MODEL_PATH)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"originalText\", StringType(), True),\n",
    "    StructField(\"lemmatized_tokens\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"I absolutely hated this book...\", [\"absolutely\", \"hate\", \"book\", \"plot\", \"be\", \"non\", \"existent\", \"character\", \"be\", \"flat\"]),\n",
    "    (\"What an amazing journey!...\", [\"amazing\", \"journey\", \"not\", \"can\", \"put\", \"down\", \"last\", \"page\"]),\n",
    "    (\"I hate it! It's awful\", [\"hate\", \"awful\"])\n",
    "]\n",
    "\n",
    "test_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "results = loaded_model.transform(test_df)\n",
    "\n",
    "results.select(\"lemmatized_tokens\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f14f24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "+----------------------------------------------------------------------+----------+\n",
    "|lemmatized_tokens                                                     |prediction|\n",
    "+----------------------------------------------------------------------+----------+\n",
    "|[absolutely, hate, book, plot, be, non, existent, character, be, flat]|0.0       |\n",
    "|[amazing, journey, not, can, put, down, last, page]                   |1.0       |\n",
    "|[hate, awful]                                                         |0.0       |\n",
    "+----------------------------------------------------------------------+----------+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182a547",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Results on Books dataset (20+ GB) for the previous version (The \"Yes Man Problem\"):\n",
    "\n",
    "+-------------------------------------------------------------------------------------+----------+\n",
    "|reviewText                                                                           |prediction|\n",
    "+-------------------------------------------------------------------------------------+----------+\n",
    "|I absolutely hated this book, the plot was non-existent and the characters were flat.|1.0       |\n",
    "|What an amazing journey! I couldn't put it down until the last page.                 |1.0       |\n",
    "|I hate it! It's awful                                                                |0.0       |\n",
    "+-------------------------------------------------------------------------------------+----------+\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
